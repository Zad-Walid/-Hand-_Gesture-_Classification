# Hand-_Gesture-_Classification Using MediaPipe Landmarks from the HaGRID Dataset.

## Overview

This project focuses on classifying hand gestures using landmark data generated by **MediaPipe** from the **HaGRID (Hand Gesture Recognition Image Dataset)**. The input is a CSV file containing hand landmarks (`x`, `y`, `z` coordinates) and gesture labels. The final output is a trained machine learning model capable of accurately recognizing hand gestures.

The project mainly consists of:
- Data preprocessing and visualization
- Training multiple ML models (SVM , XG boost and Random Forest)
- Hyperparammeter tuning for the models to get the best results available
- Evaluating and comparing model performance
- Presenting results through code and an output video 

---

## Dataset Details

- **Source**: HaGRID Dataset
- **Classes**: 18 distinct hand gestures
- **Features**: 21 landmarks per hand (each with `x`, `y`, `z` coordinates)
  
---

## Project Deliverables

### 1. Jupyter Notebook
- Containing the code and the implementation

### 2. ðŸŽ¥ Output Video
- **Demo Link**: [ðŸ”— Google Drive Video Link Here]
- A short video demonstrating real-time gesture recognition using the trained model and MediaPipe hand tracking.

---

## ðŸ“Œ Requirements

- Python
- MediaPipe
- NumPy, pandas, matplotlib
- scikit-learn
- Jupyter Notebook

---


